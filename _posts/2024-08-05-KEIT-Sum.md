---
title: "[논문리뷰] Key-Element-Informed sLLM Tuning for Document Summarization (Interspeech, 2024)"
date: 2024-08-05 14:00:00 +09:00
---

# [Paper Review] Key-Element-Informed sLLM Tuning for Document Summarization (Interspeech, 2024)

# Abstract
- easy accessibility와 low cost 장점이 있는 sLLM을 사용한 text summarization으로 변화. (from LLM)
- 하지만, 특히 긴 문서를 입력으로 주었을 때 중요 정보 누락의 문제가 있음.
- 그래서 key-element-informed instruction tuning for summarization (KEIT-Sum)을 제안한다.
    - 중요한 키워드를 캡처하여 요약을 생성함.
    - 대화나 뉴스 데이터셋에서의 실험은 KEIT-Sum을 사용한 sLLM이 (관련성이 높고 누락이 적은) 고품질의 요약을 생성함을 보인다.
- Index Terms: natural language generation, abstractive spoken document summarization, named entity recognition

# 3. Key-element-informed tuning
- 우선, 두 개의 개별 모델을 사용하여 named entities와 결론 문장으로 식별된 두 개의 별개의 key element(핵심 요소)를 추출한다.
    - named entities: 문서나 텍스트에서 중요한 정보를 제공하는 고유명사를 식별하기 위한 요소
        - ex
            - 사람 이름: 예를 들어, "Albert Einstein"이나 "Elon Musk" 같은 고유명사.
            - 조직 이름: "Google", "UN" (United Nations) 같은 회사나 기관의 이름.
            - 지리적 위치: "서울", "뉴욕", "한강" 같은 지명이나 장소 이름.
            - 날짜: "2024년 8월 1일", "금요일" 같은 날짜 표현.
            - 수량: "100달러", "50킬로미터" 같은 양적 정보.
        - named entities는 텍스트 내에서 중요한 정보 단위로서, 요약 생성 시 모델이 이러한 엔티티에 초점을 맞추도록 도와주며, 전체적인 문맥을 유지하면서 간결한 요약을 만드는 데 기여
- 그리고, 추출된 요소에 집중하도록 모델을 유도하기 위해 sLLM에서 instruction tuning을 수행하여 요약을 생성한다.

## 3.1 Key-element extraction
### Entity extraction.
- entity extraction에 NER 메커니즘을 사용한다.
- named entities 추출을 위해, 대화와 요약 데이터셋 모두에 나타난 entities의 비율을 계산한다.
    ![image](https://github.com/user-attachments/assets/b1b42757-f936-40ff-9f95-f17a62b5dc5d)
    - named entity가 요약에 빈번하게 나타나면, 그 named entity는 요약에 포함되어야함을 의미하는 것이다.
- 그러므로, 요약의 30% 이상에 나타난 named entities를 선택한다.
    - 이전 연구[16]와 같이, 뉴스 도메인에 적절한 person, date, organization, and event entity를 사용하였다.
- 각 domain에 적절한 entity를 추출한 후, 그 entity 주위에 `<and>`라는 토큰을 둘러서 강조한다. 
    - 이전 연구[16]에서는 entity의 의미를 나열했는데, KEIT-Sum 방식에서는 단순히 토큰을 통해 (entity의 실제 의미보다) 그 자체로 중요하다는 것을 강조한다.


### Conclusion extraction.
- 문서에서 주요 문장을 추출하기 위해, pre-trained BERT-based 추출 요약기[27]를 사용하고 top-1 문장을 선택한다.
- 선택된 문장을 명시적으로 전달하는 대신, 지시문을 설계할 때 문서에서 해당 문장을 단순히 표시한다.
    - 구체적으로, `<conclusion>`과 `</conclusion>` 토큰으로 감싸서 주요 문장을 강조한다.
    - 이렇게 표시된 주요 포인트를 사용하여 요약을 결론짓도록 암묵적으로 유도함으로써 더 집중된 요약을 생성할 수 있다.

![image](https://github.com/user-attachments/assets/b6942b5a-9233-4e58-992f-953e38914293)


## 3.2 Instruction tuning
- 이 방법에서는 sLLM의 fine-tuning을 위한 프롬프트로, key element가 반영된 문서와 reference summarization을 instruction과 함께 제공한다.
    - instruction: 작업의 정의와 이후에 나오는 원본 문서에서 key element가 어떻게 강조되는지를 설명함.
- 누락된 정보나 entity를 다루는 것은 잠재적으로 환각(hallucinations)을 초래할 수 있기 때문에, 우리는 이러한 문제를 완화하기 위해 자세한 지침에서 정확한 생성을 명확히 요구핬다.
    - instruction (i), converted document (d′), reference summarization (s)을 연결하여 프롬프트([i; d′; s])를 구성한다.
    - 설계된 프롬프트를 사용하여 sLLM을 fine-tuning한다.

## 4. Experimental setup
### Datasets.
- 데이터셋
    - DialogSum dataset
    - CNN/Daily Mail (CNN/DM) dataset
        - 기존 CNN/DM의 안좋은 품질 문제 해결을 위해, element-aware test set을 사용.

### Models.
- entity를 추출: well-designed NER인 Flair[30]
- key sentence 추출: BERT 요약기[27]
- sLLM 모델: LLaMA2 [32] 7B
- fine-tuning: LoRA [33]
    - r=8, dropout=0.05, alpha=32, and epoch=3
- 비교 모델: robust encoder-decoder model (BART, T5, PEGASUS, and BRIO)

### Evaluation metrics.
- 요약에서 생략 및 환각의 정도를 정확히 측정하기 위해, 우리는 UniEval [11]과 인간 평가를 사용하여 다차원 평가를 수행하고, 요약의 일관성 문제를 조사하기 위해 ChatGPT 평가 [35]를 사용.
    - UniEval: 자연어 생성(NLG) 작업을 위한 최근에 제안된 다차원 평가 도구로, 오픈 소스 다차원 평가 지표 중에서 인간 평가와의 상관관계가 가장 높음.
    - 이는 참조 요약에 과도하게 의존하지 않으면서 설명 가능한 네 가지 평가 차원(일관성, 일치성, 유창성, 관련성)을 제공
- 모델이 생성한 요약에서 환각의 정도를 측정하기 위해, 최근 도입된 ChatGPT 평가 [35, 36]를 사용
- 마지막으로, 인간 평가를 수행.

# Reference
[16] Y. Wang et al., “Element-aware summarization with large language models: Expert-aligned evaluation and chain-of-thought method,” in Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics, 2023.

[27] D. Miller, “Leveraging bert for extractive text summarization on lectures,” 2019.

[30] A. Akbik, Bergmann et al., “FLAIR: An easy-to-use framework for state-of-the-art NLP,” in Annual Conference of the North American Chapter of the Association for Computational Linguistics (Demonstrations), 2019.

[32] Touvron et al., “Llama 2: Open foundation and fine-tuned chat models,” arXiv preprint arXiv:2307.09288, 2023.

[33] E. Hu et al., “Lora: Low-rank adaptation of large language models,” arXiv preprint arXiv:2106.09685, 2021.

[35] C.-H. Chiang and oth, “Can large language models be an alternative to human evaluations?” in Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), 2023.