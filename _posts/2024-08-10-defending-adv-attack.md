---
title: "[Paper Review] DataFreeShield: Defending Adversarial Attacks without Training Data (ICML, 2024)"
date: 2024-08-10 22:30:00 +09:00
---

# Abstract
이 논문은 "데이터 없이 적대적 견고성(adversarial robustness)을 달성하는 문제"를 다루고 있다. 최근의 연구들은 적대적 견고성을 높이기 위해 대규모의 훈련 데이터 세트를 사용하는데 의존하고 있다. 그러나 실제 환경에서는 훈련 데이터가 보안이나 개인정보 보호 문제로 인해 비공개로 유지되는 경우가 많고, 대중에게는 사전 훈련된 가중치만 제공되는 상황이 일반적이다. 이러한 상황에서는 기존의 데이터 접근이 가능한 방법들이 사용할 수 없게 된다.

이 논문에서는 **DataFreeShield**라는 새로운 방법을 제안하여 원본 데이터를 전혀 접근할 수 없는 상황에서도 적대적 견고성을 확보하려고 한다. 이를 위해 두 가지 관점에서 문제를 해결하고자 한다:

1. **대체 데이터셋 생성**: 실제 데이터를 사용할 수 없으므로, 이를 대신할 수 있는 대체 데이터셋을 생성한다.
2. **생성된 데이터를 활용한 적대적 훈련**: 생성된 대체 데이터를 사용하여 모델을 적대적 공격에 대해 훈련시킨다.

논문에서는 사전 연구를 통해 원본 데이터 없이 적대적 견고성을 확보하는 것이 매우 어려운 문제임을 강조하고 있으며, 특히 비슷한 도메인의 데이터셋을 사용하더라도 원본 데이터 없이 견고성을 유지하는 것이 쉽지 않음을 보여준다. DataFreeShield는 이러한 문제를 해결하는 첫 번째 완전한 데이터 없는(data-free) 솔루션을 제시하며, 다양한 검증을 통해 기존 방법들보다 우수한 성능을 보임을 입증하고 있다.

# Data-free Adversarial Robustness Problem

## Motivational Study

<img width="413" alt="image" src="https://github.com/user-attachments/assets/c4aeae93-430e-4bdf-90af-35de57846a33">

이 이미지에는 논문에서 제안된 연구 문제와 동기 부여 실험을 설명한다.

### (a) 동기부여 연구 시나리오
- **Training Data(훈련 데이터)**: 원본 데이터(예: 의료 이미지 데이터)를 사용해 사전 학습된 네트워크(모델)를 만들었다. 그러나 이 원본 데이터는 보안 또는 개인정보 보호 문제로 인해 일반적으로 비공개되어 접근할 수 없다.
- **Malicious Attacker(악의적인 공격자)**: 공격자가 사전 학습된 모델에 대해 적대적 공격을 시도할 때, 원본 데이터에 접근할 수 없기 때문에 모델의 견고성이 저하되어 잘못된 예측을 하게 된다(빨간색으로 표시된 부분).
- **Available Solution(가능한 해결책)**: 사전 학습된 모델을 사용하여 적대적 훈련을 수행하기 위해, 비슷한 도메인에서 얻은 대체 데이터셋을 활용할 수 있다. 이를 통해 모델이 적대적 공격에 대해 더 견고하도록 훈련할 수 있다.

### (b) 유사한 도메인 데이터셋으로 훈련한 경우의 견고성 정확도
- 이 그림에서는 다양한 생물의학 데이터셋(Tissue, Blood, Path, OrganC, CIFAR)을 사용해 공격 데이터를 구성하고, 각 데이터셋에 대해 모델이 얼마나 견고하게 작동하는지를 보여준다. 
- **열린 칸**: 공격 데이터(Training Data, Attack Data)와 사용된 데이터셋(Available Data for AT)을 표시한다.
- **수치**: 각 데이터셋에 대해 적대적 훈련을 수행한 후의 견고성 정확도를 나타낸다.
  - 예를 들어, Tissue 데이터로 공격을 받았을 때, 같은 Tissue 데이터로 훈련한 경우 견고성 정확도가 37.53%였고, 다른 Blood 데이터로 훈련한 경우 견고성 정확도가 9.09%로 낮아진다.

이 실험을 통해, 같은 도메인이나 유사한 데이터셋으로 훈련하는 경우에도 견고성의 차이가 크다는 것을 보여준다. 이는 원본 데이터 없이 적대적 견고성을 확보하는 것이 매우 어려운 문제임을 시사한다.

# DataFreeShield: Learning Data-free Adversarial Robustness
<img width="849" alt="image" src="https://github.com/user-attachments/assets/31fe38b8-0ece-43db-ad9b-16099fe7659d">


### (a) Synthetic Data Generation

DataFreeShield의 첫 번째 단계는 **합성 데이터 생성**입니다. 이 단계에서는 원본 데이터 없이 무작위 노이즈 벡터를 사용하여 모델을 훈련시킬 수 있는 합성 데이터를 생성합니다.

- **입력 노이즈 벡터**는 정규 분포를 따릅니다: 
  \[
  z \sim \mathcal{N}(0, 1)
  \]
  
- 이 노이즈 벡터는 사전 학습된 네트워크 \( T_{\theta} \)에 입력되어 **합성 샘플** \( \hat{x} \)가 생성됩니다.
  
  \[
  \hat{x} = T_{\theta}(z)
  \]

- 합성 샘플은 여러 손실 함수를 통해 최적화됩니다:
  \[
  L_{Synth} = \sum_{i=1}^{n} \alpha_i L_{Synth_i}, \quad \alpha_i \sim \mathcal{U}(0, 1)
  \]

- 이 최적화는 역전파(Backpropagation)를 통해 이루어집니다.

### (b) Adversarial Training

DataFreeShield의 두 번째 단계는 **적대적 훈련**입니다. 생성된 합성 데이터를 사용하여 모델이 적대적 공격에 대해 더 견고하도록 훈련합니다.

- 먼저, 합성 샘플 \( \hat{x} \)가 타겟 모델 \( T_{\theta} \)에 입력됩니다.
  
  \[
  T_{\theta}(\hat{x})
  \]

- 이 샘플에 적대적 공격을 가하여 변형된 샘플 \( \hat{x}' \)를 생성합니다.

- 모델은 변형된 샘플에 대해 손실 \( L_{DFShield} \)을 계산하고, 이를 역전파하여 모델을 업데이트합니다.

  \[
  L_{DFShield} = L(T_{\theta}(\hat{x}), S_{\theta}(\hat{x}')) + \text{GradRefine}(\hat{x}, \hat{x}')
  \]

  여기서 GradRefine은 손실을 더 정밀하게 조정하는 모듈입니다.